{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importujemy potrzebne biblioteki"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### autorzy: Adrian Bloch i Witold Jagiełło"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# banknoty BNN - klasyfikacja autentyczności"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "device = T.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wykorzystujemy CPU jako domyślny procesor do obliczeń"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class BanknoteDataset(T.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, src_file, num_rows=None):\n",
    "        all_data = np.loadtxt(src_file, max_rows=num_rows,\n",
    "                              usecols=range(1, 6), delimiter=\"\\t\", skiprows=0,\n",
    "                              dtype=np.float32)\n",
    "\n",
    "        self.x_data = T.tensor(all_data[:, 0:4],\n",
    "                               dtype=T.float32).to(device)\n",
    "        self.y_data = T.tensor(all_data[:, 4],\n",
    "                               dtype=T.float32).to(device)\n",
    "        self.y_data = self.y_data.reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if T.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        preds = self.x_data[idx, :]\n",
    "        lbl = self.y_data[idx, :]\n",
    "        sample = {'predictors': preds, 'target': lbl}\n",
    "\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ładujemy dane z pliku potem zamieniamy je na tensory PyTorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def accuracy(model, ds):\n",
    "\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "\n",
    "\n",
    "    for i in range(len(ds)):\n",
    "        inpts = ds[i]['predictors']\n",
    "        target = ds[i]['target']\n",
    "        with T.no_grad():\n",
    "            oupt = model(inpts)\n",
    "\n",
    "        if target < 0.5 and oupt < 0.5:\n",
    "            n_correct += 1\n",
    "        elif target >= 0.5 and oupt >= 0.5:\n",
    "            n_correct += 1\n",
    "        else:\n",
    "            n_wrong += 1\n",
    "\n",
    "    return (n_correct * 1.0) / (n_correct + n_wrong)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ds : iterowalnym zestawem danych tensorów\n",
    "### Tworzy a następnie wylicza DeadLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def acc_coarse(model, ds):\n",
    "    inpts = ds[:]['predictors']\n",
    "    targets = ds[:]['target']\n",
    "    with T.no_grad():\n",
    "        oupts = model(inpts)\n",
    "    pred_y = oupts >= 0.5\n",
    "    num_correct = T.sum(targets == pred_y)\n",
    "    acc = (num_correct.item() * 1.0 / len(ds))\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### wszystkie rzędy, target 0s i 1s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def my_bce(model, batch):\n",
    "    sum = 0.0\n",
    "    inpts = batch['predictors']\n",
    "    targets = batch['target']\n",
    "    with T.no_grad():\n",
    "        oupts = model(inpts)\n",
    "    for i in range(len(inpts)):\n",
    "        oupt = oupts[i]\n",
    "\n",
    "        if targets[i] >= 0.5:\n",
    "            sum += T.log(oupt)\n",
    "        else:\n",
    "            sum += T.log(1 - oupt)\n",
    "\n",
    "    return -sum / len(inpts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### output powinien uważać na log(0), który oznacza nieskonczoność"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class Net(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hid1 = T.nn.Linear(4, 8)  # 4-(8-8)-1\n",
    "        self.hid2 = T.nn.Linear(8, 8)\n",
    "        self.oupt = T.nn.Linear(8, 1)\n",
    "\n",
    "        T.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        T.nn.init.zeros_(self.hid1.bias)\n",
    "        T.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "        T.nn.init.zeros_(self.hid2.bias)\n",
    "        T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        T.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = T.tanh(self.hid1(x))\n",
    "        z = T.tanh(self.hid2(z))\n",
    "        z = T.sigmoid(self.oupt(z))\n",
    "        return z"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tworzymy klasę Net korzystającą z nn.Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"\\nBanknote authentication using PyTorch \\n\")\n",
    "    T.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "\n",
    "\n",
    "    print(\"Creating Banknote train and test DataLoader \")\n",
    "\n",
    "    train_file = \".\\\\Data\\\\banknote_k20_train.txt\"\n",
    "    test_file = \".\\\\Data\\\\banknote_k20_test.txt\"\n",
    "\n",
    "    train_ds = BanknoteDataset(train_file)  # all rows\n",
    "    test_ds = BanknoteDataset(test_file)\n",
    "\n",
    "    bat_size = 10\n",
    "    train_ldr = T.utils.data.DataLoader(train_ds,\n",
    "                                        batch_size=bat_size, shuffle=True)\n",
    "\n",
    "\n",
    "    print(\"Creating 4-(8-8)-1 binary NN classifier \")\n",
    "    net = Net().to(device)\n",
    "\n",
    "    \"\"\"Trening\"\"\"\n",
    "    print(\"\\nPreparing training\")\n",
    "    net = net.train()  # set training mode\n",
    "    lrn_rate = 0.01\n",
    "    loss_obj = T.nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = T.optim.SGD(net.parameters(),\n",
    "                            lr=lrn_rate)\n",
    "    max_epochs = 100\n",
    "    ep_log_interval = 10\n",
    "    print(\"Loss function: \" + str(loss_obj))\n",
    "    print(\"Optimizer: SGD\")\n",
    "    print(\"Learn rate: 0.01\")\n",
    "    print(\"Batch size: 10\")\n",
    "    print(\"Max epochs: \" + str(max_epochs))\n",
    "\n",
    "    print(\"\\nStarting training\")\n",
    "    for epoch in range(0, max_epochs):\n",
    "        epoch_loss = 0.0  # for one full epoch\n",
    "        epoch_loss_custom = 0.0\n",
    "        num_lines_read = 0\n",
    "\n",
    "        for (batch_idx, batch) in enumerate(train_ldr):\n",
    "            X = batch['predictors']  # [10,4]  inputs\n",
    "            Y = batch['target']  # [10,1]  targets\n",
    "            oupt = net(X)  # [10,1]  computeds\n",
    "\n",
    "            loss_val = loss_obj(oupt, Y)  # a tensor\n",
    "            epoch_loss += loss_val.item()  # accumulate\n",
    "\n",
    "            optimizer.zero_grad()  # reset all gradients\n",
    "            loss_val.backward()  # compute all gradients\n",
    "            optimizer.step()  # update all weights\n",
    "\n",
    "        if epoch % ep_log_interval == 0:\n",
    "            print(\"epoch = %4d   loss = %0.4f\" % \\\n",
    "                  (epoch, epoch_loss))\n",
    "\n",
    "    print(\"Done \")\n",
    "\n",
    "    net = net.eval()\n",
    "    acc_train = accuracy(net, train_ds)\n",
    "    print(\"\\nAccuracy on train data = %0.2f%%\" % \\\n",
    "          (acc_train * 100))\n",
    "    acc_test = accuracy(net, test_ds)\n",
    "    print(\"Accuracy on test data = %0.2f%%\" % \\\n",
    "          (acc_test * 100))\n",
    "\n",
    "\n",
    "    print(\"\\nSaving trained model state_dict \\n\")\n",
    "    path = \"models.pth\"\n",
    "    T.save(net.state_dict(), path)\n",
    "\n",
    "\n",
    "    raw_inpt = np.array([[4.4, 1.8, -5.6, 3.2]],\n",
    "                        dtype=np.float32)\n",
    "    norm_inpt = raw_inpt / 20\n",
    "    unknown = T.tensor(norm_inpt,\n",
    "                       dtype=T.float32).to(device)\n",
    "\n",
    "    print(\"Setting normalized inputs to:\")\n",
    "    for x in unknown[0]:\n",
    "        print(\"%0.3f \" % x, end=\"\")\n",
    "\n",
    "    net = net.eval()\n",
    "    with T.no_grad():\n",
    "        raw_out = net(unknown)  # a Tensor\n",
    "    pred_prob = raw_out.item()  # scalar, [0.0, 1.0]\n",
    "\n",
    "    print(\"\\nPrediction prob = %0.6f \" % pred_prob)\n",
    "    if pred_prob < 0.5:\n",
    "        print(\"Prediction = authentic\")\n",
    "    else:\n",
    "        print(\"Prediction = forgery\")\n",
    "\n",
    "    print(\"\\nEnd Banknote demo\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tworzymy obiekty Dataset i DataLoader\n",
    "### wskazujemy pliki do testów i trenowania\n",
    "### tworzymy sieć nieuronową i wskazujemy jak działa trening\n",
    "### oceniamy model, zapisujemy go i wykonujemy przewidywanie\n",
    "###"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Banknote authentication using PyTorch \n",
      "\n",
      "Creating Banknote train and test DataLoader \n",
      "Creating 4-(8-8)-1 binary NN classifier \n",
      "\n",
      "Preparing training\n",
      "Loss function: BCELoss()\n",
      "Optimizer: SGD\n",
      "Learn rate: 0.01\n",
      "Batch size: 10\n",
      "Max epochs: 100\n",
      "\n",
      "Starting training\n",
      "epoch =    0   loss = 77.0978\n",
      "epoch =   10   loss = 41.6841\n",
      "epoch =   20   loss = 14.1492\n",
      "epoch =   30   loss = 7.9745\n",
      "epoch =   40   loss = 5.9164\n",
      "epoch =   50   loss = 4.8247\n",
      "epoch =   60   loss = 4.3570\n",
      "epoch =   70   loss = 3.9109\n",
      "epoch =   80   loss = 3.7746\n",
      "epoch =   90   loss = 3.6212\n",
      "Done \n",
      "\n",
      "Accuracy on train data = 99.09%\n",
      "Accuracy on test data = 99.27%\n",
      "\n",
      "Saving trained model state_dict \n",
      "\n",
      "Setting normalized inputs to:\n",
      "0.220 0.090 -0.280 0.160 \n",
      "Prediction prob = 0.291901 \n",
      "Prediction = authentic\n",
      "\n",
      "End Banknote demo\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### startujemy"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
